{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0620cbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from mytransformers import TransformerEncoderDecoderModel, ModelTrainer\n",
    "from mytransformers import layers\n",
    "import torch.nn as nn\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eadf3d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mytransformers import AttentionKVCacheCore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86cc497e",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 5000\n",
    "MAX_LEN = 128\n",
    "\n",
    "PAD_TOKEN_ID = 0\n",
    "UNK_TOKEN_ID = 1\n",
    "BOS_TOKEN_ID = 2\n",
    "EOS_TOKEN_ID = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "277d101d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victor/anaconda3/envs/torch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download TinyStories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since roneneldan/TinyStories couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /home/victor/.cache/huggingface/datasets/roneneldan___tiny_stories/default/0.0.0/f54c09fd23315a6f9c86f9dc80f725de7d8f9c64 (last modified on Sun Aug 24 01:43:30 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save in tinystories.txt\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Download TinyStories...\")\n",
    "dataset = load_dataset(\"roneneldan/TinyStories\",\n",
    "                       split=f\"train[:{VOCAB_SIZE}]\")\n",
    "\n",
    "with open(\"tinystories.txt\", \"w\") as f:\n",
    "    for example in dataset:\n",
    "        text = example[\"text\"].strip().lower()\n",
    "        f.write(text + \"\\n\")\n",
    "\n",
    "print(\"Save in tinystories.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd648c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Токенизатор обучен: tiny_tokenizer.model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input='tinystories.txt',\n",
    "    model_prefix='tiny_tokenizer',\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    model_type='bpe',\n",
    "    character_coverage=1.0,\n",
    "    pad_id=PAD_TOKEN_ID,\n",
    "    unk_id=UNK_TOKEN_ID,\n",
    "    bos_id=BOS_TOKEN_ID,\n",
    "    eos_id=EOS_TOKEN_ID,\n",
    "    minloglevel=2 \n",
    ")\n",
    "\n",
    "print(\"Токенизатор обучен: tiny_tokenizer.model\")\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"tiny_tokenizer.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd2e9b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class StoryDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_len=64):\n",
    "        self.texts = [t.strip().lower() for t in texts]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.pad_id = 0\n",
    "        self.bos_id = 2\n",
    "        self.eos_id = 3\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        ids = self.tokenizer.encode(text, out_type=int)\n",
    "\n",
    "        if len(ids) > self.max_len - 2:\n",
    "            ids = ids[:self.max_len - 2]\n",
    "\n",
    "        input_ids = [self.bos_id] + ids + [self.eos_id]\n",
    "        target_ids = [self.bos_id] + ids + [self.eos_id]\n",
    "\n",
    "        padding = [self.pad_id] * (self.max_len - len(input_ids))\n",
    "        input_ids += padding\n",
    "        target_ids += padding\n",
    "\n",
    "        return {\n",
    "            \"source\": torch.tensor(input_ids),\n",
    "            \"target\": torch.tensor(target_ids)\n",
    "        }\n",
    "\n",
    "texts = [ex[\"text\"] for ex in dataset]\n",
    "story_dataset = StoryDataset(texts, sp, max_len=MAX_LEN + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a795fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LAYERS = 1\n",
    "HIDDEN_STATE = 256\n",
    "\n",
    "# Attention\n",
    "NUM_QUERY_HEADS = 4\n",
    "NUM_KV_HEADS = 2\n",
    "QK_DIM = 32\n",
    "V_DIM = 64\n",
    "\n",
    "# FFN\n",
    "FFN_DIM = 48\n",
    "\n",
    "# MoE\n",
    "\n",
    "NUM_EXPERTS = 4\n",
    "K=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c0f8212",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "positional_encoding = layers.PositionalEncoding(hidden_state=HIDDEN_STATE, max_len=MAX_LEN)\n",
    "\n",
    "self_attn = layers.SelfAttention(\n",
    "    hidden_state=HIDDEN_STATE,\n",
    "    num_query_heads=NUM_QUERY_HEADS,\n",
    "    num_kv_heads=NUM_KV_HEADS,\n",
    "    qk_dim=QK_DIM,\n",
    "    v_dim=V_DIM\n",
    ")\n",
    "\n",
    "cross_attn = layers.CrossAttention(\n",
    "    hidden_state=HIDDEN_STATE,\n",
    "    num_query_heads=NUM_QUERY_HEADS,\n",
    "    num_kv_heads=NUM_KV_HEADS,\n",
    "    qk_dim=QK_DIM,\n",
    "    v_dim=V_DIM\n",
    ")\n",
    "\n",
    "expert = layers.FeedForward(\n",
    "    hidden_state=HIDDEN_STATE,\n",
    "    ffn_dim=FFN_DIM\n",
    ")\n",
    "\n",
    "gate = layers.FeedForward(\n",
    "    hidden_state=HIDDEN_STATE,\n",
    "    ffn_dim=NUM_EXPERTS,\n",
    "    func=nn.Softmax(dim=-1)\n",
    "    \n",
    ")\n",
    "ffn = layers.MoELayer(\n",
    "    num_experts=NUM_EXPERTS,\n",
    "    k=K,\n",
    "    expert_model=expert,\n",
    "    gate_model=gate\n",
    ")\n",
    "\n",
    "encoder_layer = layers.TransformerEncoderLayer(\n",
    "    hidden_state=HIDDEN_STATE,\n",
    "    attn_model=self_attn,\n",
    "    ffn_model=ffn\n",
    ")\n",
    "\n",
    "decoder_layer = layers.TransformerDecoderLayer(\n",
    "    hidden_state=HIDDEN_STATE,\n",
    "    self_attn_model=self_attn,\n",
    "    ffn_model=ffn,\n",
    "    encoder_output=True,\n",
    "    cross_attn_model=cross_attn\n",
    ")\n",
    "model = TransformerEncoderDecoderModel(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    pad_token_id=PAD_TOKEN_ID,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    hidden_state=HIDDEN_STATE,\n",
    "    positional_encoding_model=positional_encoding,\n",
    "    encoder_layer=encoder_layer,\n",
    "    decoder_layer=decoder_layer\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c888868",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5361bc75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:08<00:00, 75.82batch/s, Epoch Loss=4.7364]     \n"
     ]
    }
   ],
   "source": [
    "trainer = ModelTrainer(\n",
    "    num_epochs=1,\n",
    "    batch_size=8,\n",
    "    pad_token_id=0 \n",
    ")\n",
    "\n",
    "test_losses = trainer.train(\n",
    "    data=story_dataset,\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion\n",
    ")\n",
    "\n",
    "# График\n",
    "# trainer.print_loss_history(path_save=\"loss_plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a63e4ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сгенерировано: dmgdmg gfdgk;d. one day,\n"
     ]
    }
   ],
   "source": [
    "def generate(model, tokenizer, prompt, max_len=100, device='cuda'):\n",
    "    model.eval()\n",
    "    tokens = tokenizer.encode(prompt.lower(), out_type=int)\n",
    "    tokens = [2] + tokens  # <bos>\n",
    "    model.generate(tokens, tokens, 3, max_len)\n",
    "        \n",
    "\n",
    "    return tokenizer.decode(tokens)\n",
    "\n",
    "# Пример\n",
    "prompt = \"dmgdmg gfdgk;d \"\n",
    "gen = generate(model, sp, prompt, max_len=30)\n",
    "print(\"Сгенерировано:\", gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6cbed676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kv cache status:True\n",
      "kv cache status:True\n",
      "kv cache status:True\n",
      "kv cache status:False\n",
      "kv cache status:False\n",
      "kv cache status:False\n"
     ]
    }
   ],
   "source": [
    "AttentionKVCacheCore.call_kv_cache_method(model, \"print_kv_cache_status\") \n",
    "AttentionKVCacheCore.call_kv_cache_method(model, \"disable_kv_cache\") \n",
    "AttentionKVCacheCore.call_kv_cache_method(model, \"print_kv_cache_status\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd3d894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сгенерировано: dog were two friends were two friends. they were playing in they were they were they were they were they were they were they were they were they were they\n"
     ]
    }
   ],
   "source": [
    "def generate(model, tokenizer, prompt, max_len=100, device='cuda'):\n",
    "    model.eval()\n",
    "    tokens = tokenizer.encode(prompt.lower(), out_type=int)\n",
    "    tokens = [2] + tokens  # <bos>\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        src = torch.tensor([tokens]).to(device)\n",
    "        tgt = torch.tensor([tokens]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(src, tgt)\n",
    "            next_token_logits = logits[0, -1, :]  # последний токен\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1).item()\n",
    "\n",
    "        if next_token == 3:  # <eos>\n",
    "            break\n",
    "        tokens.append(next_token)\n",
    "\n",
    "    return tokenizer.decode(tokens)\n",
    "\n",
    "# Пример\n",
    "prompt = \"dmgdmg gfdgk;d \"\n",
    "gen = generate(model, sp, prompt, max_len=30)\n",
    "print(\"Сгенерировано:\", gen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
